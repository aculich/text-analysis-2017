{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging using NLTK\n",
    "\n",
    "One task in NLP has been to reliably identify a word's part of speech. This can help us with the ever-present task of identifying content words, but can be used in a variety of analyses. Part-of-speech tagging is a specific instance in the larger category of word tagging, or placing words in pre-determined categories.\n",
    "\n",
    "Today we'll learn how to identify a word's part of speech and think through reasons we may want to do this.\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "* Understand the intuition behind tagging and information extraction\n",
    "* Use NLTK to tag the part of speech of each word\n",
    "* Count most frequent words based on their part of speech\n",
    "\n",
    "### Outline\n",
    "* [Part-of-Speech Tagging](#pos)\n",
    "* [Counting words based on their part of speech](#counting)\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "* *part-of-speech tagging*: \n",
    "    * the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context\n",
    "* *named entity recognition*:\n",
    "    * a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc\n",
    "* *tree* \n",
    "    * data structure made up of nodes or vertices and edges without having any cycle. \n",
    "* *treebank*:\n",
    "    * a parsed text corpus that annotates syntactic or semantic sentence structure\n",
    "* *tuple*:\n",
    "    * a sequence of immutable Python objects\n",
    "\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "For more information on information extraction using NLTK, see chapter 7: http://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pos'></a>\n",
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that stop words are typically short function words. Intuitively, if we could identify the part of speech of a word, we would have another way of identifying content words. NLTK can do that too!\n",
    "\n",
    "NLTK has a function that will tag the part of speech of every token in a text. For this, we will re-create our original tokenized text sentence from Monday, with the stop words and punctuation.\n",
    "\n",
    "NLTK uses the Penn Treebank Project to tag the part-of-speech of the words. The NLTK algoritm is deterministic - it assigns the most common part of speech for each word, as found in the Penn Treebank. You can find a list of all the part-of-speech tags here:\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"For me it has to do with the work that gets done at the crossroads of \\\n",
    "digital media and traditional humanistic study. And that happens in two different ways. \\\n",
    "On the one hand, it's bringing the tools and techniques of digital media to bear \\\n",
    "on traditional humanistic questions; on the other, it's also bringing humanistic modes \\\n",
    "of inquiry to bear on digital media.\"\n",
    "\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "#check we did everything correctly\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the nltk pos function to tag the tokens\n",
    "tagged_sentence_tokens = nltk.pos_tag(sentence_tokens)\n",
    "\n",
    "#view tagged sentence\n",
    "tagged_sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes more complicated code. Stay with me. The above output is a list of *tuples*. A tuple is a sequence of Python objects. In this case, each of these tuples is a sequence of strings. To loop through tuples is intuitively the same as looping through a list, but slightly different syntax. \n",
    "\n",
    "Note that this is not a list of lists, as we saw in our lesson on Pandas. This is a list of tuples.\n",
    "\n",
    "Let's pull out the part-of-speech tag from each tuple above and save that to a list. Notice the order stays exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tags = [tag for (word, tag) in tagged_sentence_tokens]\n",
    "print(word_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the difference in syntax for the above code compared to our standard list comprehension code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='counting'></a>\n",
    "### Counting words based on their part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the part-of-speech tags in a similar way we counted words, to output the most frequent types of words in our text. We can also count words based on their part of speech.\n",
    "\n",
    "First, we count the frequency of each part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_frequency = nltk.FreqDist(word_tags)\n",
    "tagged_frequency.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence contains a lot of adjectives. So let's first look at the adjectives. Notice the syntax here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = [word for (word,pos) in tagged_sentence_tokens if pos == 'JJ' or pos=='JJR' or pos=='JJS']\n",
    "\n",
    "#print all of the adjectives\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [word for (word,pos) in tagged_sentence_tokens if pos=='NN' or pos=='NNS']\n",
    "\n",
    "#print all of the nouns\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbs = [word for (word,pos) in tagged_sentence_tokens if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "verbs = [word for (word,pos) in tagged_sentence_tokens if pos in ['VB', 'VBD','VBG','VBN','VBP','VBZ']]\n",
    "\n",
    "#print all of the verbs\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ex: Print the most frequent nouns, adjective, and verbs in the sentence\n",
    "######What does this tell us?\n",
    "######Compare this to what we did yesterday with removing stop words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

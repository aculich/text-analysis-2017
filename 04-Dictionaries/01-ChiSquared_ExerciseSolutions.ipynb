{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Computing chi-squared and (non-normalized) correlation statistics\n",
    "\n",
    "As a bonus, we will calculate the chi-squared statistic for all of the words in two novels, *Pride and Prejudice* and *Garland for Girls*, and then calculate the non-normalized correlation for two sample words in the corpus. This can be used, for example, to derive dictionaries from labeled text, as is done in \n",
    "\n",
    "Jacob Jensen, Ethan Kaplan, Suresh Naidu, and Laurence Wilse-Samson (2012).\n",
    "[Political Polarization and the Dynamics of Political Language: Evidence from 130 Years of Partisan Speech.](https://www.brookings.edu/bpea-articles/political-polarization-and-the-dynamics-of-political-language-evidence-from-130-years-of-partisan-speech/) Brookings Papers on Economic Activity.\n",
    "\n",
    "\n",
    "Don't worry if you don't understand all of this. If it helps some of you, great. If it's a bit advanced with the math no problem. Stick with me as much as you can.\n",
    "\n",
    "### 0. Document Term Matrix\n",
    "First, I'll create a document term matrix from the two novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_list = []\n",
    "#open and read the novels, save them as variables\n",
    "austen_string = open('../Data/Austen_PrideAndPrejudice.txt', encoding='utf-8').read()\n",
    "alcott_string = open('../Data/Alcott_GarlandForGirls.txt', encoding='utf-8').read()\n",
    "\n",
    "#append each novel to the list\n",
    "text_list.append(austen_string)\n",
    "text_list.append(alcott_string)\n",
    "\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "\n",
    "novels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chi-Squared for sample words\n",
    "\n",
    "The chi-sqaured statistic is a simple calculation:\n",
    "\n",
    "![alt text](http://www.statisticshowto.com/wp-content/uploads/2013/09/chi-square-formula.jpg \"Chi Squared\")\n",
    "\n",
    "Let's look at the frequency for two words, \"sister\" and \"child\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df[['sister', 'child']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the chi-squared statistic for these two words we need to know the expected frequency, if these two words were used the same in both novels. To do this, we divide the total frequency across both novels by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_sister = novels_df['sister'].sum(axis=0)/2\n",
    "expected_sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_child = novels_df['child'].sum(axis=0)/2\n",
    "expected_child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the chi_squares we subtract the expected frequency for each novel from the actual frequency for each novel, square this value, and divide by the expected frequency, and add the two numbers together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sister = ((novels_df.loc[0,'sister'] - expected_sister)**2 / expected_sister) + ((novels_df.loc[1,'sister'] - expected_sister)**2 / expected_sister)\n",
    "chi_sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_child = ((novels_df.loc[0,'child'] - expected_child)**2 / expected_child) + ((novels_df.loc[1,'child'] - expected_child)**2 / expected_child)\n",
    "chi_child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are large values. Let's try a word that has a much closer frequency across both novels. The result is a much smaller chi-squared statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df['writing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_writes = novels_df['writing'].sum(axis=0)/2\n",
    "chi_writes = ((novels_df.loc[0,'writing'] - expected_writes)**2 / expected_writes) + ((novels_df.loc[1,'writing'] - expected_writes)**2 / expected_writes)\n",
    "chi_writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Partisan Score\n",
    "\n",
    "Next, we can find the partisan score for our chosen words. We do this simply, by multiplying the word frequency in *Pride and Prejudice* by 1, and multiple the word frequency in *Garland for Girls* by -1, and adding these together. A partisan score above 0 will indicate it's used more often in Austen, a negative score will mean it is used more often in Alcott."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sister_corr = novels_df.loc[0,'sister']*1 + novels_df.loc[1,'sister']*-1\n",
    "sister_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_corr = (novels_df.loc[0,'child']*1) + (novels_df.loc[1,'child']*-1)\n",
    "child_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_corr = (novels_df.loc[0,'writing']*1) + (novels_df.loc[1,'writing']*-1)\n",
    "writing_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writes_corr = (novels_df.loc[0,'writes']*1) + (novels_df.loc[1,'writes']*-1)\n",
    "writes_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a partisan score of 0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df['writes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chi-Squared for every word, using a for-loop\n",
    "\n",
    "Now we can calculate this for each word in our corpus. To do this we have to introduce the for loop. We've seen this before in list comprehension, but we're splitting it out now into multiple lines. To think this intuitively, take this example:\n",
    "\n",
    "For every child that knocks on my door on Halloween I will do the following:\n",
    "1. Ask them what their costume is\n",
    "2. Give them a piece of candy\n",
    "3. Cackle wildly\n",
    "\n",
    "The for loop in Python is intuitively the same. For every element in a list, we want to do something to that element.\n",
    "\n",
    "In this case, we will loop through all columns in our dataframe and calculate the chi-squared statistic. We will then append both the column name (our word) and the chi-squared statistic to a list using .append()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(novels_df)\n",
    "chi_list = []\n",
    "\n",
    "for c in columns:\n",
    "    chi_list.append([c,((novels_df.loc[0,c] - novels_df[c].sum(axis=0)/2)**2 / novels_df[c].sum(axis=0)/2) + ((novels_df.loc[1,c] - novels_df[c].sum(axis=0)/2)**2 / novels_df[c].sum(axis=0)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sort this list by the second element in each tuple (it's not technically a tuple, but no matter), and print the top 50 \"partisan\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_list.sort(key=lambda x: x[1], reverse=True)\n",
    "chi_list[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Calculate the partisan score for each word in the corpus and print the most partisan words for each novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = list(novels_df)\n",
    "pearson_list = []\n",
    "\n",
    "for c in columns:\n",
    "    pearson_list.append([c,((novels_df.loc[0,c]*1) + (novels_df.loc[1,c]*-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words most associated with Jane Austen\n",
    "pearson_list.sort(key=lambda x: x[1], reverse=True)\n",
    "pearson_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words most associated with Alcott\n",
    "pearson_list.sort(key=lambda x: x[1], reverse=False)\n",
    "pearson_list[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

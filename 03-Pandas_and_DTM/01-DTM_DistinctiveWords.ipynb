{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Document Term Matrix and Finding Distinctive Words\n",
    "\n",
    "We have been dealing with texts as strings, or as lists of strings. Another way to represent text which opens up a variety of other possibilities for analysis is the Document Term Matrix (DTM).\n",
    "\n",
    "The best Python library for this, along with the subsequent analyses we can peform on a DTM, is scikit-learn. It's a powerful library, and one you will continually return to as you advance in text analysis (and looks great on your CV!). At it's core, this library allow us to implement a variety of machine learning algorithms on our text.\n",
    "\n",
    "Because scikit-learn is such a large and powerful library the goal today is not to become experts, but instead learn the basic functions in the library and gain an intuition about how you might use it to do text analysis. To give an overview, here are some of the things you can do using scikit-learn:\n",
    "* word weighting\n",
    "* feature extraction\n",
    "* text classification / supervised machine learning\n",
    "    * L2 regression\n",
    "    * classification algorithms such as nearest neighbors, SVM, and random forest\n",
    "* clustering / unsupervised machine learning\n",
    "    * k-means\n",
    "    * pca\n",
    "    * cosine similarity\n",
    "    * LDA\n",
    "\n",
    "Today, we'll start with the Document Term Matrix (DTM). The DTM is the bread and butter of most computational text analysis techniques, both simple and more sophisticated methods. In this lesson we will use Python's scikit-learn package learn to make a document term matrix from a .csv Music Reviews dataset (collected from MetaCritic.com). We will visualize the DTM in a pandas dataframe. We will then use the DTM and a word weighting technique called tf-idf (term frequency inverse document frequency) to identify important and discriminating words within this dataset. The illustrating question: what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums? Finally, we'll use the DTM to implement a difference of proportions calculation on two novels in our data folder.\n",
    "\n",
    "  \n",
    "\n",
    "### Learning Goals\n",
    "* Understand the DTM and why it's important to text analysis\n",
    "* Learn how to create a DTM from a .csv file\n",
    "* Learn basic functionality of Python's package scikit-learn\n",
    "* Understand tf-idf scores, and word scores in general\n",
    "* Learn a simple way to identify distinctive words\n",
    "\n",
    "\n",
    "### Outline\n",
    "<ol start=\"0\">\n",
    "  <li>[The Pandas Dataframe: Music Reviews](#df)</li>\n",
    "  <li>[Explore the Data using Pandas](#stats)</li>\n",
    "          -Basic descriptive statistics\n",
    "  <li>[Creating the DTM: scikit-learn](#dtm)</li>\n",
    "          -CountVectorizer function\n",
    "  <li>[What can we do with a DTM?](#counting)</li>\n",
    "  <li>[Tf-idf scores](#tfidf)</li>\n",
    "          -TfidfVectorizer function\n",
    "  <li>[Identifying Distinctive Words 1](#distinct)</li>\n",
    "          -Application: Identify distinctive words by genre\n",
    "   <li>[Identifying Distinctive Words 2](#dop)</li>\n",
    "          -Difference of Proportions    \n",
    "         \n",
    "</ol>\n",
    "\n",
    "### Key Terms\n",
    "* *Document Term Matrix*:\n",
    "    * a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "* *TF-IDF Scores*: \n",
    "    *  short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "    \n",
    "### Further Resources\n",
    "\n",
    "[This blog post](https://de.dariah.eu/tatom/feature_selection.html) goes through finding distinctive words using Python in more detail.\n",
    "\n",
    "Paper: [Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf), Burt Monroe, Michael Colaresi, Kevin Quinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df'></a>\n",
    "### 0. The Pandas Dataframe: Music Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read a music reviews corpus, which is stored as a .csv file on our hard drive, into a Pandas dataframe. These data were collected from MetaCritic.com, and include all user reviews of albums from the start of the website through 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "#create a dataframe called \"df\"\n",
    "df = pandas.read_csv(\"../Data/BDHSI2016_music_reviews.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "\n",
    "#view the dataframe\n",
    "#The column \"body\" contains our text of interest.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first review from the column 'body'\n",
    "df.loc[0,'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stats'></a>\n",
    "### 1. Explore the Data using Pandas\n",
    "\n",
    "You folks are experts at this now. Write Python code using pandas to do the following exploration of the data:\n",
    "\n",
    "1. What different genres are in the data?\n",
    "2. Who are the reviewers?\n",
    "3. Who are the artists?\n",
    "4. What is the average score given?\n",
    "5. What is the average score by genre? What is the genre with the highest average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtm'></a>\n",
    "### 2. Creating the DTM: scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's the summary of the metadata. Next, we turn to analyzing the text of the reviews. Remember, the text is stored in the 'body' column. First, a preprocessing step to remove numbers. To do this we will use a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to turn the text into a document term matrix using the scikit-learn function called CountVectorizer. There are two ways to do this. We can turn it into a sparse matrix type, which can be used within scikit-learn for further analyses. We do this using the fit_transform() function from CountVectorizer.\n",
    "\n",
    "[Let's first look at the documentation for CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "sklearn_dtm = countvec.fit_transform(df.body)\n",
    "print(sklearn_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what each number indicates? We can access the words themselves through the CountVectorizer function get_feature_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countvec.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. It save a lot of memory to store the dtm in this format, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first convert this matrix back to a Pandas dataframe, a format we're more familiar with.\n",
    "\n",
    "Note: This is a case of do as I say, not as I do. As we continue we will rarely transform a DTM into a Pandas dataframe, because of memory issues. I'm doing it today so we can understand the intuition behind the DTM, word scores, and distinctive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we do the same as we did above, but covert it into a Pandas dataframe. Note this takes quite a bit more memory, so will not be good for bigger data.\n",
    "#don't understand this code? we'll go through it, but don't worry about understanding it.\n",
    "dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view the dtm dataframe\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='counting'></a>\n",
    "### 3. What can we do with a DTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a number of calculations using a DTM. For a toy example, we can quickly identify the most frequent words (compare this to how many steps it took using NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm_df.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Ex: print the average number of times each word is used in a review\n",
    "#Print this out sorted from highest to lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What does this tell us about our data?\n",
    "\n",
    "What else does the DTM enable? Because it is in the format of a matrix, we can perform any matrix algebra or vector manipulation on it, which enables some pretty exciting things (think vector space and Euclidean geometry). But, what do we lose when we reprsent text in this format?\n",
    "\n",
    "Today, we will use variations on the DTM to find distinctive words in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "### 4. Tf-idf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to find content words in a corpus is a long-standing question in text analysis. We have seen a few ways of doing this: removing stop words and identifying and counting only nouns, verbs, and adjectives. Today, we'll learn one more simple approach to this: word scores. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be indicative of the content of that document. We want to instead identify frequent words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is *tf-idf* scores. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'; what we have been calling stop words.\n",
    "\n",
    "More precisely, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. We'll use it, but a challenge for you: use Pandas to calculate this manually. \n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "\n",
    "#create the dtm, but with cells weigthed by the tf-idf score.\n",
    "dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view results\n",
    "dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm_tfidf_df.max().sort_values(ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words and without part-of-speech tagging. What else do you notice about this list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distinct'></a>\n",
    "### 5. Identifying Distinctive Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add the genre of the document into our dtm weighted by tf-idf scores, and then compare genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copy our tfidf df to a new df to add genre\n",
    "dtm_tfidf_df_genre = dtm_tfidf_df\n",
    "#add a 'GENRE' column to our tfidf df\n",
    "dtm_tfidf_df_genre['GENRE'] = df['genre']\n",
    "#Question: Why is 'GENRE' in caps?\n",
    "\n",
    "dtm_tfidf_df_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest tf-idf weight for each genre. \n",
    "\n",
    "Note: there are other ways to do this. Challenge: what is a different approach to identifying rows from a certain genre in our dtm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out the reviews for three genres, Rap, Alternative/Indie Rock, and Jazz\n",
    "dtm_rap = dtm_tfidf_df_genre[dtm_tfidf_df_genre['GENRE']==\"Rap\"]\n",
    "dtm_indie = dtm_tfidf_df_genre[dtm_tfidf_df_genre['GENRE']==\"Alternative/Indie Rock\"]\n",
    "dtm_jazz = dtm_tfidf_df_genre[dtm_tfidf_df_genre['GENRE']==\"Jazz\"]\n",
    "\n",
    "#print the words with the highest tf-idf scores for each genre\n",
    "print(\"Rap Words\")\n",
    "print(dtm_rap.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Indie Words\")\n",
    "print(dtm_indie.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Jazz Words\")\n",
    "print(dtm_jazz.max(numeric_only=True).sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying content words, and distinctive words based on groups of texts. You notice there are some proper nouns in there. How might we remove those if we're not interested in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex: Compare the distinctive words for two artists in the data\n",
    "\n",
    "Note: the artists should have a number of reviews, so check your frequency counts to identify artists.\n",
    "\n",
    "HINT: Copy and paste the above code and modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dop'></a>\n",
    "### 6. Difference of proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple way to calculate distinctive words in two texts is to calculate the words with the highest and lowest difference or proportions. In theory frequent words like 'the' and 'of' will have a small difference. In practice this doesn't happen.\n",
    "\n",
    "To demonstrate this we will run a difference of proportion calculation on *Pride and Prejudice* and *A Garland for Girls*.\n",
    "\n",
    "To get the text in shape for scikit-learn we need to creat a list object with each novel as an element in a list. We'll use the append function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text_list = []\n",
    "#open and read the novels, save them as variables\n",
    "austen_string = open('../Data/Austen_PrideAndPrejudice.txt', encoding='utf-8').read()\n",
    "alcott_string = open('../Data/Alcott_GarlandForGirls.txt', encoding='utf-8').read()\n",
    "\n",
    "#append each novel to the list\n",
    "text_list.append(austen_string)\n",
    "text_list.append(alcott_string)\n",
    "print(text_list[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat a DTM from these two novels, force it into a pandas DF, and inspect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the number of rows and columns.\n",
    "\n",
    "Question: What does this mean?\n",
    "\n",
    "Next, we need to get a word frequency count for each novel, which we can do by summing across the entire row. Note how the syntax is different here compared to when we summed one column across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df['word_count'] = novels_df.sum(axis=1)\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we divide each frequency cell by the word count. This syntax gets a bit tricky, so let's walk through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df = novels_df.iloc[:,:].div(novels_df.word_count, axis=0)\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we subtract one row from another, and add the output as a third row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df.loc[2] = novels_df.loc[0] - novels_df.loc[1]\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort based of the values of this row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df.loc[2].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are still in there. Why?\n",
    "\n",
    "We can, of course, manually remove stop words. This does successfully identify distinctive content words. \n",
    "\n",
    "We can do this in the CountVectorizer step, by setting the correct option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change stop_words option to 'english\n",
    "countvec_sw = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#same as code above\n",
    "novels_df_sw = pandas.DataFrame(countvec_sw.fit_transform(text_list).toarray(), columns=countvec_sw.get_feature_names())\n",
    "novels_df_sw['word_count'] = novels_df_sw.sum(axis=1)\n",
    "novels_df_sw = novels_df_sw.iloc[:,0:].div(novels_df_sw.word_count, axis=0)\n",
    "novels_df_sw.loc[2] = novels_df_sw.loc[0] - novels_df_sw.loc[1]\n",
    "novels_df_sw.loc[2].sort_values(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this by setting the max_df option (maximum document frequency) to either an absolute value, or a decimal between 0 and 1. An absolute value indicate that if the word occurs in more documents than the stated value, that word **will not** be included in the DTM. A decimal value will do the same, but proportion of documents.\n",
    "\n",
    "Question: In the case of this corpus, what does setting the max_df value to 1 do? What output do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change max_df option to 1\n",
    "countvec_freq = CountVectorizer(max_df=1)\n",
    "\n",
    "#same as the code above\n",
    "novels_df_freq = pandas.DataFrame(countvec_freq.fit_transform(text_list).toarray(), columns=countvec_freq.get_feature_names())\n",
    "novels_df_freq['word_count'] = novels_df_freq.sum(axis=1)\n",
    "novels_df_freq = novels_df_freq.iloc[:,0:].div(novels_df_freq.word_count, axis=0)\n",
    "novels_df_freq.loc[2] = novels_df_freq.loc[0] - novels_df_freq.loc[1]\n",
    "novels_df_freq.loc[2].sort_values(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What would happen if we set the max_df to 2, in this case?\n",
    "Question: What might we do for the music reviews dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise: \n",
    "\n",
    "Use the difference of proportions calculation to compare two genres, or two artists, in the music reviews dataset. There are many ways you can do this. Think through the problem in steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
